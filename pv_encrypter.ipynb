{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set AWS Credentials\n",
    "\n",
    "If you've already set the defaults in your AWS credentials or config files, then you don't need to do anything futher. However, if you wish to use a specific set of AWS credentials/region, use the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AWS_REGION\"] = \"ap-south-1\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_utils import (\n",
    "    ec2_client,\n",
    "    create_ebs_snapshot,\n",
    "    encrypt_ebs_snapshot,\n",
    "    create_ebs_volume_from_snapshot,\n",
    "    volume_exists,\n",
    ")\n",
    "from k8s_utils import (\n",
    "    v1,\n",
    "    get_pv_list,\n",
    "    get_ebs_backed_pvs,\n",
    "    get_pod_list,\n",
    "    scale_deployment,\n",
    "    scale_stateful_set,\n",
    "    get_pvc,\n",
    "    pv_exists,\n",
    "    pvc_exists,\n",
    ")\n",
    "\n",
    "from main import (\n",
    "    logger,\n",
    "    get_unencrypted_ebs_pvs,\n",
    "    get_owners,\n",
    "    get_volume_id_short,\n",
    "    get_snapshot_details,\n",
    "    get_snapshot_list_progress,\n",
    ")\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import botocore\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start/Resume a Run\n",
    "\n",
    "If you want to resume a run, make sure you enter the same name as before. The `run_` will be prepended automatically below.\n",
    "\n",
    "#### NOTE\n",
    "If you execute certain cells multiple times for some reason, expect weird behaviour. Make sure to change the run name in those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"run_{input()}\"\n",
    "run_folder = f\"pv_encrypter_state/{run_name}\"\n",
    "\n",
    "if Path(run_folder).exists():\n",
    "    logger.info(f\"Resuming run {run_name} from {run_folder}\")\n",
    "else:\n",
    "    Path(run_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    logger.info(f\"Starting run {run_name}\")\n",
    "    logger.info(f\"Created folder: {run_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overview\n",
    "rf\"\"\"\n",
    "              Internal Flow\n",
    "\n",
    "    Get list of all PVs with EBS Volumes      ||\n",
    "                    |                         ||\n",
    "Get list of PVs with unencrypted EBS Volumes  ||\n",
    "                    |                         ||\n",
    " Scale down linked Deployments/StatefulSets   ||\n",
    "                    |                         ||\n",
    "              Create Snapshot                 ||\n",
    "                    |                         ||\n",
    "      Create Encrypted Copy of Snapshot       ||\n",
    "                    |                         ||\n",
    "    Create EBS Volume from Encrypted Copy     ||\n",
    "                    |                         ||\n",
    "       Delete Persistent Volume Claim         ||\n",
    "                    |                         ||\n",
    "        Modify PV with new Volume ID          ||\n",
    "                    |                         ||\n",
    "                Create PV                     ||\n",
    "                    |                         ||\n",
    "                Create PVC                    ||\n",
    "                    |                        \\  /\n",
    " Scale up linked Deployments/StatefulSets     \\/\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Info (A)\n",
    "\n",
    "Regarding the PVs and Pods in the Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all Persistent Volumes in the Cluster.\n",
    "pv_list = get_pv_list()\n",
    "\n",
    "# Get all PVs that are backed by EBS Volumes.\n",
    "ebs_pv_list = get_ebs_backed_pvs(pv_list)\n",
    "\n",
    "# Find the PVs with unencrypted EBS Volumes.\n",
    "# NOTE: This is the main list we use for the rest of the notebook.\n",
    "# So the order of the `unenc_pv_list` will be maintained and used in enc_snapshot_id_ls\n",
    "# and volume_id_ls.\n",
    "unenc_pv_list = get_unencrypted_ebs_pvs(ebs_pv_list)\n",
    "\n",
    "# Get the list of all Pods.\n",
    "pod_list = get_pod_list()\n",
    "\n",
    "# Get all the owners of the pods that are linked to one of the PVs from `unenc_pv_list`\n",
    "# through a PVC.\n",
    "valid_owners = get_owners(pod_list, unenc_pv_list)\n",
    "\n",
    "# State A\n",
    "# Save the above information to disk for later use if necessary.\n",
    "with open(f\"{run_folder}/state_a.pkl\", \"wb\") as w:\n",
    "    pickle.dump((pv_list, ebs_pv_list, unenc_pv_list, pod_list, valid_owners), w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume State A\n",
    "\n",
    "If your run was interrupted after Step A was completed and you haven't proceeded with the subsequent steps, run the following cell to resume your run from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional)\n",
    "with open(f\"{run_folder}/state_a.pkl\", \"rb\") as r:\n",
    "    (pv_list, ebs_pv_list, unenc_pv_list, pod_list, valid_owners) = pickle.load(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Down Owners\n",
    "\n",
    "The Deployments and StatefulSets that are linked to the PVs we care about could have running pods.\n",
    "\n",
    "In those cases, in order to ensure data integrity, we have to scale them down before proceeding.\n",
    "\n",
    "### NOTE (Very Important)\n",
    "\n",
    "* Remember that your applications/microservices can have multiple Deployments/StatefulSets supporting them. So if just one of them is scaled down, say a MySQL DB, the rest of the application could fail.\n",
    "* If your Deployment/StatefulSet is managed by ArgoCD or its equivalent, it's possible that any modifications that this notebook makes can be overridden when the drift is detected a few minutes later. So we expect 0 replicas, while the reality could be that ArgoCD, for example, has fully scaled it up in the background.\n",
    "* The following cells **only** care about Deployments/StatefulSets. But there can be any number of owner types.\n",
    "\n",
    "In these or any one of many, many potential cases, make sure to get your hands dirty and bring down the Deployments/StatefulSets/any other owner yourself; make sure they're scaled down appropriately for the duration of this exercise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work on Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_ls = list(valid_owners[\"Deployment\"].keys())\n",
    "stateful_set_ls = list(valid_owners[\"StatefulSet\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this to limit what gets scaled down.\n",
    "# From the above two tables that are displayed at the output of the \"Collect Info\"\n",
    "# cell, choose from the left most column: \"Index\" and add it to the corresponding lists\n",
    "# below.\n",
    "# The deployments/statefulsets at these indexes will get scaled down.\n",
    "\n",
    "# The default below takes in the whole list, so you can run this cell without\n",
    "# modification if you want to operate on the full list.\n",
    "# Example index selection:\n",
    "# >> deployment_index_ls = [1, 10, 21, 37]\n",
    "# >> stateful_set_index_ls = [1, 2, 15, 29]\n",
    "\n",
    "deployment_index_ls = list(range(len(deployment_ls)))\n",
    "stateful_set_index_ls = list(range(len(stateful_set_ls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_deployment_ls = [deployment_ls[i] for i in deployment_index_ls]\n",
    "limited_stateful_set_ls = [stateful_set_ls[i] for i in stateful_set_index_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional)\n",
    "# Verify the Deployments and StatefulSets that will be scaled down.\n",
    "# Format:\n",
    "# <namespace>|<name>\n",
    "\n",
    "print(limited_deployment_ls)\n",
    "print(limited_stateful_set_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = input(\n",
    "    \"Are you sure you wish to go ahead? This will scale down deployments and statefulsets linked to the PVs that are in the process of being encrypted. Enter 'YES'.\"\n",
    ")\n",
    "\n",
    "if decision == \"YES\":\n",
    "    # Scale down Deployments.\n",
    "    for name in limited_deployment_ls:\n",
    "\n",
    "        # The name contains both namespace and name separated by a '|'.\n",
    "        deployment_namespace = name.split(\"|\")[0]\n",
    "        deployment_name = name.split(\"|\")[1]\n",
    "\n",
    "        scale_deployment(\n",
    "            name=deployment_name,\n",
    "            namespace=deployment_namespace,\n",
    "            replicas=0,\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Scaling down deployment {deployment_name} in namespace {deployment_namespace}\"\n",
    "        )\n",
    "\n",
    "    # Scale down StatefulSets.\n",
    "    for name in limited_stateful_set_ls:\n",
    "\n",
    "        # The name contains both namespace and name separated by a '|'.\n",
    "        stateful_set_namespace = name.split(\"|\")[0]\n",
    "        stateful_set_name = name.split(\"|\")[1]\n",
    "\n",
    "        scale_stateful_set(\n",
    "            name=stateful_set_name,\n",
    "            namespace=stateful_set_namespace,\n",
    "            replicas=0,\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Scaling down statefulset {stateful_set_name} in namespace {stateful_set_namespace}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Important) Limit PVs\n",
    "\n",
    "This is where you get to limit the PVs that get operated on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the PV indexes that you want to operate on.\n",
    "# You could refer to the right-most column: \"PV Index\" of the two tables in the\n",
    "# output of the \"Collect Info\" cell.\n",
    "\n",
    "# The default below takes in the whole list, so you can run this cell without\n",
    "# modification if you want to operate on the full list.\n",
    "# Example: unenc_pv_index_list = [1, 4, 21, 37]\n",
    "\n",
    "unenc_pv_index_list = list(range(len(unenc_pv_list)))\n",
    "limited_unenc_pv_list = [unenc_pv_list[i] for i in unenc_pv_index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional)\n",
    "# Review the operational list of PVs.\n",
    "# (PV Name, PVC Name, PVC Namespace)\n",
    "print([(pv.metadata.name, pv.spec.claim_ref.name, pv.spec.claim_ref.name) for pv in limited_unenc_pv_list])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Snapshots (B)\n",
    "\n",
    "Create Snapshots of all qualifying EBS volumes.\n",
    "\n",
    "### NOTE\n",
    "For any AWS account, there can a maximum of 100 pending Snapshots at any time in a region. So we do the Snapshot creation in batches of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of IDs of unencrypted Snapshots created from the unencrypted EBS volumes.\n",
    "unenc_snapshot_id_ls = []\n",
    "\n",
    "# A folder to backup the PV and PVC objects.\n",
    "# We keep this separate from the backup of the other objects because they're critical\n",
    "# to the recreation process later on. \n",
    "Path(f\"{run_folder}/pv_pvc_manifests\").mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches of 100 PVs each from the `limited_unenc_pv_list` list.\n",
    "\n",
    "unenc_pv_batches = []\n",
    "for batch_num in range((len(limited_unenc_pv_list) // 100) + 1):\n",
    "\n",
    "    # Make sure the start index is less than the length of the list.\n",
    "    if batch_num * 100 < len(limited_unenc_pv_list):\n",
    "\n",
    "        # Add a batch of 100 elements.\n",
    "        # 0:100, 100:200, 200:300, ...\n",
    "        unenc_pv_batches.append(\n",
    "            limited_unenc_pv_list[batch_num * 100 : (batch_num + 1) * 100]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each batch.\n",
    "for batch_idx, batch in enumerate(unenc_pv_batches):\n",
    "\n",
    "    logger.info(f\"Processing Batch #{batch_idx}\")\n",
    "\n",
    "    # For each PV in the list of unencrypted PVs.\n",
    "    for idx, pv in enumerate(batch):\n",
    "\n",
    "        logger.info(f\"Iteration {idx}\")\n",
    "\n",
    "        # Get the PVC linked to the PV.\n",
    "        pvc = get_pvc(\n",
    "            claim_ref_name=pv.spec.claim_ref.name,\n",
    "            claim_ref_namespace=pv.spec.claim_ref.namespace,\n",
    "        )\n",
    "\n",
    "        # Save the manifest of the PV and the corresponding PVC so they can be recreated\n",
    "        # later.\n",
    "        with open(\n",
    "            f\"{run_folder}/pv_pvc_manifests/{pv.spec.claim_ref.name}.pkl\", \"wb\"\n",
    "        ) as w:\n",
    "            pickle.dump({\"pv\": pv, \"pvc\": pvc}, w)\n",
    "\n",
    "        # Strip the `aws://<region>/` prefix.\n",
    "        volume_id = get_volume_id_short(pv.spec.aws_elastic_block_store.volume_id)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Triggering Snapshot creation for {volume_id}\"\n",
    "        )\n",
    "\n",
    "        # Create the EBS snapshot from the unencrypted EBS volume.\n",
    "        snapshot_id = create_ebs_snapshot(\n",
    "            volume_id=volume_id,\n",
    "            extra_log_info=f\"Created for run: {run_name} at {str(datetime.now())}\",\n",
    "            tags=[\n",
    "                {\n",
    "                    \"Key\": \"RunName\",\n",
    "                    \"Value\": run_name,\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"In-Progress Snapshot ID: {snapshot_id}\"\n",
    "        )\n",
    "\n",
    "        unenc_snapshot_id_ls.append(snapshot_id)\n",
    "\n",
    "# State B\n",
    "# Save list of unencrypted Snapshot IDs to disk.\n",
    "with open(f\"{run_folder}/unenc_snapshot_id_ls.pkl\", \"wb\") as w:\n",
    "    pickle.dump(unenc_snapshot_id_ls, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Snapshot Status\n",
    "\n",
    "The following is a blocking cell; it will block execution until the Snapshots have been fully created in AWS.\n",
    "\n",
    "Uncomment the cell to run it. Or if you don't want to block execution, run the cell below that periodically to check for completion.\n",
    "\n",
    "In any case, do NOT proceed with the rest of the notebook without this step being complete and it's confirmed that all snapshots are completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional)\n",
    "# Blocking cell.\n",
    "status = False\n",
    "while not status:\n",
    "    response = get_snapshot_list_progress(unenc_snapshot_id_ls)\n",
    "    if response[\"avg_progress\"] == 100:\n",
    "        status = True\n",
    "    else:\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Blocking Cell to ensure all Snapshots in `unenc_snapshot_id_ls` are complete.\n",
    "response = get_snapshot_list_progress(unenc_snapshot_id_ls)\n",
    "assert (\n",
    "    response[\"avg_progress\"] == 100\n",
    "), f\"Progress is still at {response['avg_progress']}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume State B\n",
    "\n",
    "If your run was interrupted after Step B was completed and you haven't proceeded with the subsequent steps, run the following cell to resume your run from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load State A\n",
    "with open(f\"{run_folder}/state_a.pkl\", \"rb\") as r:\n",
    "    (pv_list, ebs_pv_list, unenc_pv_list, pod_list, valid_owners) = pickle.load(r)\n",
    "\n",
    "# Load State B\n",
    "with open(f\"{run_folder}/unenc_snapshot_id_ls.pkl\", \"rb\") as r:\n",
    "    unenc_snapshot_id_ls = pickle.load(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Encrypted Snapshots (C)\n",
    "\n",
    "Create encrypted copies of the snapshots created in the previous step and stored in `unenc_snapshot_id_ls`.\n",
    "\n",
    "### NOTE\n",
    "For any AWS account, there can a maximum of 20 pending Snapshots being copied at any time in a region. So we do the Snapshot copying in batches of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of encrypted Snapshots copied from the unencrypted snapshots.\n",
    "enc_snapshot_id_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches.\n",
    "enc_snapshot_id_batches = []\n",
    "for batch_num in range((len(enc_snapshot_id_ls) // 20) + 1):\n",
    "\n",
    "    # Make sure the start index is less than the length of the list.\n",
    "    if batch_num * 20 < len(enc_snapshot_id_ls):\n",
    "\n",
    "        # Add a batch of 20 elements.\n",
    "        # 0:20, 20:40, 40:60, ...\n",
    "        enc_snapshot_id_batches.append(\n",
    "            enc_snapshot_id_ls[batch_num * 20 : (batch_num + 1) * 20]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each batch.\n",
    "for batch_idx, batch in enumerate(enc_snapshot_id_batches):\n",
    "\n",
    "    logger.info(f\"Processing Batch #{batch_idx}\")\n",
    "\n",
    "    # For each PV in the list of unencrypted PVs.\n",
    "    for idx, unenc_snapshot_id in enumerate(batch):\n",
    "\n",
    "        logger.info(f\"Iteration {idx}\")\n",
    "\n",
    "        logger.info(\n",
    "            f\"Triggering encrypted Snapshot copy for {unenc_snapshot_id}.\"\n",
    "        )\n",
    "\n",
    "        # Make an encrypted copy of the unencrypted EBS snapshot.\n",
    "        snapshot_id = encrypt_ebs_snapshot(\n",
    "            snapshot_id=unenc_snapshot_id,\n",
    "            extra_log_info=f\"Created for run: {run_name} at {str(datetime.now())}\",\n",
    "            tags=[\n",
    "                {\n",
    "                    \"Key\": \"RunName\",\n",
    "                    \"Value\": run_name,\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        if snapshot_id is not False:\n",
    "            logger.info(\n",
    "                f\"In-Progress Snapshot ID: {snapshot_id}\"\n",
    "            )\n",
    "        else:\n",
    "            logger.error(f\"Encrypted Snapshot not created. Error. Snapshot: {unenc_snapshot_id}\")\n",
    "\n",
    "        enc_snapshot_id_ls.append(snapshot_id)\n",
    "\n",
    "# State C\n",
    "# Save the list of encrypted snapshots to disk.\n",
    "with open(f\"{run_folder}/enc_snapshot_id_ls.pkl\", \"wb\") as w:\n",
    "    pickle.dump(enc_snapshot_id_ls, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Snapshot Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional)\n",
    "# Blocking cell.\n",
    "status = False\n",
    "while not status:\n",
    "    response = get_snapshot_list_progress(enc_snapshot_id_ls)\n",
    "    if response[\"avg_progress\"] == 100:\n",
    "        status = True\n",
    "    else:\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Blocking Cell.\n",
    "# Check status of Snapshot creation before proceeding.\n",
    "response = get_snapshot_list_progress(enc_snapshot_id_ls)\n",
    "assert (\n",
    "    response[\"avg_progress\"] == 100\n",
    "), f\"Progress is still at {response['avg_progress']}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume State C\n",
    "\n",
    "If your run was interrupted after Step C was completed and you haven't proceeded with the subsequent steps, run the following cell to resume your run from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load State A\n",
    "with open(f\"{run_folder}/state_a.pkl\", \"rb\") as r:\n",
    "    (pv_list, ebs_pv_list, unenc_pv_list, pod_list, valid_owners) = pickle.load(r)\n",
    "\n",
    "# Load State B\n",
    "with open(f\"{run_folder}/unenc_snapshot_id_ls.pkl\", \"rb\") as r:\n",
    "    unenc_snapshot_id_ls = pickle.load(r)\n",
    "\n",
    "# Load State C\n",
    "with open(f\"{run_folder}/enc_snapshot_id_ls.pkl\", \"rb\") as r:\n",
    "    enc_snapshot_id_ls = pickle.load(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Volumes from Encrypted Snapshots (D)\n",
    "\n",
    "Using the encrypted snapshots created in the previous step, create encrypted EBS Volumes in the same AZ as the original volume.\n",
    "\n",
    "There's a one-one relationship between: `limited_unenc_pv_list`, `unenc_snapshot_id_ls`, and `enc_snapshot_id_ls`. So we use this to find the AZ of the EBS volume from the labels of the PV.\n",
    "\n",
    "NOTE: There's no delay to EBS Volume creation from a Snapshot. So there's no quota limiting us, meaning there's no need to batch our requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of encrypted Volume IDs.\n",
    "volume_id_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each PV in the list of unencrypted PVs.\n",
    "for idx, snapshot_id in enumerate(enc_snapshot_id_ls):\n",
    "\n",
    "    # Find Availability Zone. Important since the new volume should be in the same region as the old one.\n",
    "    availability_zone = limited_unenc_pv_list[idx].metadata.labels.get(\n",
    "        \"topology.kubernetes.io/zone\"\n",
    "    ) or limited_unenc_pv_list[idx].metadata.labels.get(\n",
    "        \"failure-domain.beta.kubernetes.io/zone\"\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Triggering Volume creation from {snapshot_id}\")\n",
    "\n",
    "    # Create EBS Volume from the encrypted Snapshot.\n",
    "    volume_id = create_ebs_volume_from_snapshot(\n",
    "        snapshot_id=snapshot_id,\n",
    "        availability_zone=availability_zone,\n",
    "        tags=[\n",
    "            {\n",
    "                \"Key\": \"RunName\",\n",
    "                \"Value\": run_name,\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if volume_id is not False:\n",
    "        logger.info(\n",
    "            f\"Created Volume ID: {volume_id}\"\n",
    "        )\n",
    "    else:\n",
    "        logger.error(f\"Volume not created. Error. Snapshot: {snapshot_id}\")\n",
    "\n",
    "    volume_id_ls.append(volume_id)\n",
    "\n",
    "# Save the list of encrypted Volume IDs to disk.\n",
    "with open(f\"{run_folder}/volume_id_ls.pkl\", \"wb\") as w:\n",
    "    pickle.dump(volume_id_ls, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume Status\n",
    "\n",
    "Volumes are immediately created and don't have a waiting period before they can be used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume State D\n",
    "\n",
    "If your run was interrupted, run the following cell to resume your run from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load State A\n",
    "with open(f\"{run_folder}/state_a.pkl\", \"rb\") as r:\n",
    "    (pv_list, ebs_pv_list, unenc_pv_list, pod_list, valid_owners) = pickle.load(r)\n",
    "\n",
    "# Load State B\n",
    "with open(f\"{run_folder}/unenc_snapshot_id_ls.pkl\", \"rb\") as r:\n",
    "    unenc_snapshot_id_ls = pickle.load(r)\n",
    "\n",
    "# Load State C\n",
    "with open(f\"{run_folder}/enc_snapshot_id_ls.pkl\", \"rb\") as r:\n",
    "    enc_snapshot_id_ls = pickle.load(r)\n",
    "\n",
    "# Load State D\n",
    "with open(f\"{run_folder}/volume_id_ls.pkl\", \"rb\") as r:\n",
    "    volume_id_ls = pickle.load(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning\n",
    "\n",
    "Till this point, we've not had any destructive operations, but the next step, which is PVC deletion, will delete the linked PV, which will in turn (if the PV is configured thusly) delete the underlying EBS volume.\n",
    "\n",
    "Points to remember:\n",
    "\n",
    "* For each EBS volume, there are:\n",
    "    * Unencrypted snapshots.\n",
    "    * Encrypted snapshots.\n",
    "    * A new EBS volume created from the encrypted copy.\n",
    "\n",
    "So, you're **definitely not** risking your data itself. It's safe in these three forms. You can further confirm this by checking the resources in AWS, observing the tags applied to each one, etc.\n",
    "\n",
    "However, exercise caution beyond this point. I highly recommend that you try with a single PV/PVC before running it for everything."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Saved PVs and PVCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of unencrypted PVs.\n",
    "len(limited_unenc_pv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Modify this to see information about different PV/PVC pairs.\n",
    "idx = 0\n",
    "\n",
    "pv = limited_unenc_pv_list[idx]\n",
    "\n",
    "with open(f\"{run_folder}/pv_pvc_manifests/{pv.spec.claim_ref.name}.pkl\", \"rb\") as r:\n",
    "    pv_pvc_manifests = pickle.load(r)\n",
    "\n",
    "print(\"PVC Name:\", pv_pvc_manifests[\"pvc\"].metadata.name)\n",
    "print(\"PV Name:\", pv_pvc_manifests[\"pv\"].metadata.name)\n",
    "print(\"PV Spec Claim Reference:\", pv_pvc_manifests[\"pv\"].spec.claim_ref)\n",
    "print(\"PV EBS Spec:\", pv_pvc_manifests[\"pv\"].spec.aws_elastic_block_store)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Cleaned PVCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_list = []\n",
    "for pv in limited_unenc_pv_list:\n",
    "    with open(f\"{run_folder}/pv_pvc_manifests/{pv.spec.claim_ref.name}.pkl\", \"rb\") as r:\n",
    "        pv_pvc_manifests = pickle.load(r)\n",
    "\n",
    "    # List of tuples (pv, pvc)\n",
    "    # These are PV and PVC objects which have been stripped of any temporary keys like creation time, uid, etc.\n",
    "    cleaned_list.append((pv_pvc_manifests[\"pv\"], pv_pvc_manifests[\"pvc\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete PVC -> Create PV -> Create PVC\n",
    "\n",
    "As mentioned earlier, deleting the original PVC will also delete the linked PV, which will then delete the associated EBS volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"{len(cleaned_list)} PVs/PVCs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (pv, pvc) in enumerate(cleaned_list):\n",
    "\n",
    "    logger.info(f\"Iteration {idx}\")\n",
    "\n",
    "    # Delete PVC.\n",
    "\n",
    "    logger.info(f\"Deleting PVC {pvc.metadata.name}\")\n",
    "\n",
    "    try:\n",
    "        response = v1.delete_namespaced_persistent_volume_claim(\n",
    "            name=pvc.metadata.name,\n",
    "            namespace=pvc.metadata.namespace,\n",
    "        )\n",
    "    except:\n",
    "        logger.info(\"Error during PVC deletion. Continuing with the next iteration.\")\n",
    "        continue\n",
    "\n",
    "    logger.info(f\"Deleted PVC {pvc.metadata.name}\")\n",
    "\n",
    "    # Provide a small delay for the PVC deletion to propagate to the PV.\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Create PV.\n",
    "\n",
    "    logger.info(\n",
    "        f\"Creating PV {pv.metadata.name} with the newly encrypted volume: {volume_id_ls[idx]}\"\n",
    "    )\n",
    "\n",
    "    # Set the volume_id to the encrypted one that we just created.\n",
    "    try:\n",
    "        pv.spec.aws_elastic_block_store.volume_id = volume_id_ls[idx]\n",
    "        pv_creation_response = v1.create_persistent_volume(body=pv)\n",
    "    except:\n",
    "        logger.info(\"Error during PV creation. Continuing with the next iteration.\")\n",
    "        continue\n",
    "\n",
    "    logger.info(f\"Created Persistent Volume: {pv.metadata.name}\")\n",
    "\n",
    "    # Create PVC.\n",
    "\n",
    "    logger.info(f\"Creating PVC {pvc.metadata.name}.\")\n",
    "\n",
    "    # We change nothing because we've created the PV with the same name so the\n",
    "    # references still hold valid.\n",
    "    try:\n",
    "        pvc_creation_response = v1.create_namespaced_persistent_volume_claim(\n",
    "            namespace=pvc.metadata.namespace,\n",
    "            body=pvc,\n",
    "        )\n",
    "    except:\n",
    "        logger.info(\"Error during PVC creation. Continuing with the next iteration.\")\n",
    "        continue\n",
    "\n",
    "    logger.info(f\"Created Persistent Volume Claim: {pvc.metadata.name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Up Deployments/StatefulSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = input(\n",
    "    \"Are you sure you want to go ahead? This will scale up deployments and statefulsets linked to the PVs that were just encrypted. Enter 'YES'.\"\n",
    ")\n",
    "\n",
    "if decision == \"YES\":\n",
    "    # Scale down Deployments.\n",
    "    for name in limited_deployment_ls:\n",
    "\n",
    "        # The name contains both namespace and name separated by a '|'.\n",
    "        deployment_namespace = name.split(\"|\")[0]\n",
    "        deployment_name = name.split(\"|\")[1]\n",
    "\n",
    "        replicas = len(valid_owners[\"Deployment\"][name])\n",
    "        scale_deployment(\n",
    "            name=deployment_name,\n",
    "            namespace=deployment_namespace,\n",
    "            replicas=replicas,\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Scaling up deployment {deployment_name} in namespace {deployment_namespace} to {replicas} replicas.\"\n",
    "        )\n",
    "\n",
    "    # Scale down StatefulSets.\n",
    "    for name in limited_stateful_set_ls:\n",
    "\n",
    "        stateful_set_namespace = name.split(\"|\")[0]\n",
    "        stateful_set_name = name.split(\"|\")[1]\n",
    "\n",
    "        replicas = len(valid_owners[\"StatefulSet\"][name])\n",
    "        scale_stateful_set(\n",
    "            name=stateful_set_name,\n",
    "            namespace=stateful_set_namespace,\n",
    "            replicas=replicas,\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Scaling up statefulset {stateful_set_name} in namespace {stateful_set_namespace} to {replicas} replicas.\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eks-pv-encrypter-DV-S42hP-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1261b05abb22a6b55cb54394a4563c6336854bb93d9f6443b4f4be2d055c35f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
